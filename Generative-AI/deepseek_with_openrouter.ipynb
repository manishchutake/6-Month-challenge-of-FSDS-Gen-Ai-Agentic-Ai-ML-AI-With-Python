{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTNQ47rPZhwO"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# First API call with reasoning\n",
        "response = requests.post(\n",
        "  url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
        "  headers={\n",
        "    \"Authorization\": \"\",\n",
        "    \"Content-Type\": \"application/json\",\n",
        "  },\n",
        "  data=json.dumps({\n",
        "    \"model\": \"deepseek/deepseek-v3.2\",\n",
        "    \"messages\": [\n",
        "        {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": \"How many r's are in the word 'strawberry'?\"\n",
        "        }\n",
        "      ],\n",
        "    \"reasoning\": {\"enabled\": True}\n",
        "  })\n",
        ")\n",
        "\n",
        "# Extract the assistant message with reasoning_details\n",
        "response = response.json()\n",
        "response = response['choices'][0]['message']\n",
        "\n",
        "# Preserve the assistant message with reasoning_details\n",
        "messages = [\n",
        "  {\"role\": \"user\", \"content\": \"How many r's are in the word 'strawberry'?\"},\n",
        "  {\n",
        "    \"role\": \"assistant\",\n",
        "    \"content\": response.get('content'),\n",
        "    \"reasoning_details\": response.get('reasoning_details')  # Pass back unmodified\n",
        "  },\n",
        "  {\"role\": \"user\", \"content\": \"Are you sure? Think carefully.\"}\n",
        "]\n",
        "\n",
        "# Second API call - model continues reasoning from where it left off\n",
        "response2 = requests.post(\n",
        "  url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
        "  data=json.dumps({\n",
        "    \"model\": \"deepseek/deepseek-v3.2\",\n",
        "    \"messages\": messages,  # Includes preserved reasoning_details\n",
        "    \"reasoning\": {\"enabled\": True}\n",
        "  })\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvT4yD_fZ6pc",
        "outputId": "b8b52ba2-164f-4d08-d979-0dced48e3269"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'role': 'assistant', 'content': 'There are three r\\'s in the word \"strawberry.\" The spelling is s-t-r-a-w-b-e-r-r-y, with r\\'s at the third, eighth, and ninth positions.', 'refusal': None, 'reasoning': 'We are asked: \"How many r\\'s are in the word \\'strawberry\\'?\" Let\\'s count the letters in \"strawberry\". The word is spelled: s-t-r-a-w-b-e-r-r-y. So, we see: first r after t, then later there is e, then r, then r, then y. So there are three r\\'s? Let\\'s list: positions: 1: s, 2: t, 3: r, 4: a, 5: w, 6: b, 7: e, 8: r, 9: r, 10: y. Indeed, there are three r\\'s: at positions 3, 8, and 9. So answer is 3.\\n\\nBut the user might be tricking with a play on words? The question says \"How many r\\'s are in the word \\'strawberry\\'?\" It could be interpreted as how many times the letter \\'r\\' appears. That is straightforward. So answer is 3.\\n\\nI\\'ll answer: There are 3 r\\'s in \"strawberry\".', 'reasoning_details': [{'format': 'unknown', 'index': 0, 'type': 'reasoning.text', 'text': 'We are asked: \"How many r\\'s are in the word \\'strawberry\\'?\" Let\\'s count the letters in \"strawberry\". The word is spelled: s-t-r-a-w-b-e-r-r-y. So, we see: first r after t, then later there is e, then r, then r, then y. So there are three r\\'s? Let\\'s list: positions: 1: s, 2: t, 3: r, 4: a, 5: w, 6: b, 7: e, 8: r, 9: r, 10: y. Indeed, there are three r\\'s: at positions 3, 8, and 9. So answer is 3.\\n\\nBut the user might be tricking with a play on words? The question says \"How many r\\'s are in the word \\'strawberry\\'?\" It could be interpreted as how many times the letter \\'r\\' appears. That is straightforward. So answer is 3.\\n\\nI\\'ll answer: There are 3 r\\'s in \"strawberry\".'}]}\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OeR8VZMiZ9jb"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "response = requests.post(\n",
        "  url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
        "  headers={\n",
        "    \"Authorization\": \"\",\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"HTTP-Referer\": \"<YOUR_SITE_URL>\", # Optional. Site URL for rankings on openrouter.ai.\n",
        "    \"X-Title\": \"<YOUR_SITE_NAME>\", # Optional. Site title for rankings on openrouter.ai.\n",
        "  },\n",
        "  data=json.dumps({\n",
        "    \"model\": \"nex-agi/deepseek-v3.1-nex-n1:free\",\n",
        "    \"messages\": [\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"What is the meaning of life?\"\n",
        "      }\n",
        "    ]\n",
        "  })\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYPY3DlWbi_t",
        "outputId": "f6fd348d-90ac-4e9a-d4ea-40e281a560b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 'gen-1766413206-Ie63l8UVCNkgse0FsF3w', 'provider': 'SiliconFlow', 'model': 'nex-agi/deepseek-v3.1-nex-n1:free', 'object': 'chat.completion', 'created': 1766413206, 'choices': [{'logprobs': None, 'finish_reason': 'stop', 'native_finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': \"The meaning of life is one of humanity's most profound and enduring questions. Different perspectives offer various answers:\\n\\n**Philosophical perspectives:**\\n- **Existentialism**: Life has no inherent meaning; we create our own meaning through choices and actions\\n- **Absurdism**: We must find meaning despite life's inherent lack of it\\n- **Humanism**: Meaning comes from human flourishing, relationships, and contributing to society\\n\\n**Religious/spiritual views:**\\n- Fulfilling divine purpose or spiritual growth\\n- Achieving enlightenment or union with the divine\\n- Following moral/ethical principles\\n\\n**Scientific perspective:**\\n- Biological: Survival and reproduction of genes\\n- Evolutionary: Life is a natural process without predetermined purpose\\n\\n**Personal meaning:**\\nMany find meaning through:\\n- Relationships and love\\n- Creativity and self-expression\\n- Helping others and making a positive impact\\n- Learning, growth, and self-actualization\\n- Pursuit of knowledge and truth\\n\\nUltimately, the question may be less about discovering a universal meaning and more about how we choose to live meaningfully. Different answers resonate with different people, and many find meaning through multiple sources.\", 'refusal': None, 'reasoning': None}}], 'system_fingerprint': '', 'usage': {'prompt_tokens': 597, 'completion_tokens': 232, 'total_tokens': 829, 'cost': 0, 'is_byok': False, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0, 'video_tokens': 0}, 'cost_details': {'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}, 'completion_tokens_details': {'reasoning_tokens': 0, 'image_tokens': 0}}}\n"
          ]
        }
      ],
      "source": [
        "print(response.json())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysb-R9tPcp-V"
      },
      "source": [
        "# grok -- xai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajdBYHnhbyoC"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "response = requests.post(\n",
        "  url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
        "  headers={\n",
        "    \"Authorization\": \"\",\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"HTTP-Referer\": \"<YOUR_SITE_URL>\", # Optional. Site URL for rankings on openrouter.ai.\n",
        "    \"X-Title\": \"<YOUR_SITE_NAME>\", # Optional. Site title for rankings on openrouter.ai.\n",
        "  },\n",
        "  data=json.dumps({\n",
        "    \"model\": \"x-ai/grok-code-fast-1\",\n",
        "    \"messages\": [\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"can you predict about future job recession about stock or job?\"\n",
        "      }\n",
        "    ]\n",
        "  })\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0YZ-o1ddY5Q",
        "outputId": "cb881530-4e24-452c-bd7e-74e86b82b647"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 'gen-1766413573-T01RyA1wyalsih5NhpFp', 'provider': 'xAI', 'model': 'x-ai/grok-code-fast-1', 'object': 'chat.completion', 'created': 1766413573, 'choices': [{'logprobs': None, 'finish_reason': 'stop', 'native_finish_reason': 'completed', 'index': 0, 'message': {'role': 'assistant', 'content': '### Clarifying Your Question\\nIt seems like you\\'re asking for predictions on future job recessions, possibly linked to stock markets or job markets (e.g., Will there be a recession causing job losses, and how does the stock market factor in?). I\\'m not a crystal ball, so I\\'ll base this on current economic data, trends, and expert analyses as of late 2023. Recessions are hard to predict precisely, but I\\'ll break it down into key areas.\\n\\n### Current Economic Context\\n- **Global and US Economy**: The US has avoided a broad recession for now after the 2021-2022 inflation spike and pandemic fallout. GDP growth has slowed (e.g., annualized Q3 2023 growth around 5%, per Bureau of Economic Analysis), driven by tech sector corrections, supply chain issues, and rising interest rates from the Federal Reserve to curb inflation (now at ~3.1%, per latest CPI reports).\\n- **Job Market Status**: Unemployment is low at ~3.7% (October 2023, Bureau of Labor Statistics), with 2.7 million jobs added in 2023 so far. However, layoffs in tech (e.g., Meta, Amazon, Google cutting ~150,000 roles) signal sector-specific pressures, often tied to overinvestment during the pandemic boom.\\n- **Stock Market Pulse**: US markets have been volatile; the S&P 500 fell ~20% from early 2022 peaks but recovered some (closing 2023 with modest gains). High valuations and debt levels make stocks sensitive to rate hikes or economic shocks.\\n\\nEvidence for optimism: Historical data (e.g., Federal Reserve models) shows the US often soft-lands recessions, with job markets recovering faster than in previous decades.\\n\\n### Likelihood of a Job Recession Soon\\nA \"job recession\" typically means widespread unemployment rising above 5-6%, often coinciding with economic contraction. Predictions from sources like JPMorgan, Goldman Sachs, and Moody\\'s Analytics (2023 reports) suggest:\\n- **Near-Term Odds**: Low immediate risk of a full recession. Economists (e.g., via Bloomberg consensus) peg a 30-40% chance for 2024, down from early 2023 peaks. The Fed\\'s tightening cycle (funds rate at 5.25-5.5%) is likely to end soon, potentially stabilizing things.\\n- **Job-Specific Risks**: Layoffs could rise if inflation persists or consumer spending drops (e.g., housing market cooling with mortgage rates ~7%). Tech, finance, and retail are most vulnerable; AI advancements may replace jobs but also create new ones.\\n- **Stock Market Link**: Recessions often start with stock plunges (e.g., 2008 Lehman collapse preceded massive job losses). Currently, stocks are a leading indicator—if they drop 10-20% sustainably, watch for job impacts. Inverse correlation: Falling stocks can signal reduced investment, leading to cuts.\\n\\n### Long-Term Predictions (Next 5-10 Years)\\n- **Cyclic Patterns**: US recessions average every 5-7 years (per NBER data); last one ended in April 2020. Demographic shifts (aging workforce) and globalization could moderate job volatility.\\n- **Potential Triggers**: Climate crises, geopolitical tensions (e.g., Ukraine/China), or a hard Fed landing could spark one. Bullish view: AI and green energy might boost jobs, mitigating risks.\\n- **Expert Forecasts**: International Monetary Fund projects global growth slowing to ~3% 2024-2028, with US job growth at ~1% annually. Risks: If inequality or debt crises escalate, job recessions could hit harder in disadvantaged sectors.\\n\\n### What You Can Do\\n- Monitor indicators like the yield curve (currently inverted, a recession predictor), consumer confidence, and PMI surveys.\\n- Diversify investments: Stocks have recovered from past recessions (e.g., S&P up 15x since 1982 lows), but jobs depend on skills—reskilling in tech/AI is key.\\n- Sources for updates: Fed Economic Projections, BLS reports, or tools like Google Trends for real-time data.\\n\\nIf this isn\\'t what you meant, could you rephrase? Economic predictions evolve fast.', 'refusal': None, 'reasoning': 'First, the question is: \"can you predict about future job recession about stock or job?\" This seems unclear. It might be asking for predictions on a job recession, possibly related to stock markets or job markets. Perhaps it\\'s a typo or poorly worded: \"future job recession about stock or job\" – maybe \"future job recession for stock or job markets\"?\\n\\n\\n## Interpreting the question  \\n- The query asks, \"can you predict about future job recession about stock or job?\"  \\n- It seems to request predictions on a job recession, possibly linked to stock markets or job markets.  \\n- The phrasing may be unclear, potentially a typo for \"future job recession for stock or job markets.\"  \\n- Let\\'s check if the intent is to forecast economic trends in employment or stocks.\\n\\n## Clarifying the Intent  \\n- The query likely asks for predictions on future job recessions, possibly tied to stocks or job markets.', 'reasoning_details': [{'format': 'xai-responses-v1', 'index': 0, 'type': 'reasoning.summary', 'summary': 'First, the question is: \"can you predict about future job recession about stock or job?\" This seems unclear. It might be asking for predictions on a job recession, possibly related to stock markets or job markets. Perhaps it\\'s a typo or poorly worded: \"future job recession about stock or job\" – maybe \"future job recession for stock or job markets\"?\\n\\n\\n## Interpreting the question  \\n- The query asks, \"can you predict about future job recession about stock or job?\"  \\n- It seems to request predictions on a job recession, possibly linked to stock markets or job markets.  \\n- The phrasing may be unclear, potentially a typo for \"future job recession for stock or job markets.\"  \\n- Let\\'s check if the intent is to forecast economic trends in employment or stocks.\\n\\n## Clarifying the Intent  \\n- The query likely asks for predictions on future job recessions, possibly tied to stocks or job markets.'}, {'id': 'rs_59b94e9a-f3b3-b551-4b2c-c30cf2d14825', 'format': 'xai-responses-v1', 'index': 0, 'type': 'reasoning.encrypted', 'data': 'MflpyTrGE+J/SNTcMYhoHMeOa0TEUkmdGTsvRWMd4yAbaHek7V2fACaC1yRx2ttafHDLlRf5lDMC5e44oQ4vU+L0m6ekBmg32ud6IBlYz+3ZYpTneMlKzVE0U6tt7t13Em/uxlk+9+jsU8sLgLoai92m8chXGl3+BEMWrDNC+oneP/Id6BxTrY8IwsxRS5ReymFMRNa1PHxanRteJQbBR1UCvnFNfq0Vyy3JNzG3TREwltTydVuroz4D36vDlN2c+AZJNfeOdyrrzB2IRit4AIELTwMBbKLFTpz1kpAfQXR/G6f7uEDC/Jzzt5bRTeVCDPF/RtnaEoDXz5FGNfTaIfTxEXYyvDvPSGC5Qbjk6klV+FgW67jlLfaC6JUqNsYAtWgRRbGpjgj6tzUhS5mVjWYE+dEANHXguz8Hx3BVIHK2RXaeHgDc3mF0de7LuRAbXUnkwiq8hbaVsExDUknAGBOvA1EyPT+Ns2Gb785X76IaUC1cYspkuwzQH5ytz/gInkspVudSd9+j4U9I6m2Lz3UeyNtw34jROHFF1UccFqzceurM2FDBFCQPQYsxn9MtLqpTKFu2IHClGQbAogwGKeZMl7vMwgQjqHP2Q4jSlYM9oSD394rnsbzyzdrhAyGNo+dvL4GQnjLRwysAXDKRQvbK0DRDCgksPweDtW5ojBNh11CkmmBPuXr61NdValUMQePSpNYisCc48da454BclB0ehOdyb1v4Gk/rc05OkTq/y+GNt1i7Hh3pZw5UJ+7WAqKA921cdo2LQsT/FjGqm1h9NpVsu1obI+2+wzytAnJoWa5M5mBUmri1PO6H1H5T9Xr5tlcwf9jtcGhtF62QPHxIAv2kxE3/Ma7M3j9l0gO8R7L4L4NMrWePDoRM9fw3cIvicer0RasMCgyJMai1v2WPv/WmWbjGN6EmM9klBqicG1pBaSjAdmbP154KSqVKry2/TIK+30QObksX4WHR+9aSVC2XmoVeVGPOjsEvBMu3GBViaNU6T4l1vs4BlT7vPmDDwTKi99TYdCZY7LxD629SQJDbXwy9aWxeBjnc0UUzeZHgSJ23CezM86lCIat71dWR3Ar818ZlZEnn+7LwZ/vqCpoyQfVg9P1WWndTzOfpXknvZy81vj8ZVXzvaa+pszaNmOfSfwgZv1ZQjpy+ycFYgzjm+bdbALHAUNpEwB3joIBxQpGcz9+70HT8e47BzSRQ2ROaWL+Abped+jHMI8p6WRehozCSljR64U2cV7TsE4S9fQiVlkwcGO1BbimDAKaoRJ0MSW39IGDS0zMJhPlvnjh7Ci+QZ7ySZMwsNjDDOtxVz2t4x1PkG3Ol7Xpn+OfRLOxGvH24txWTzIn9aemALodCI5NYryr+eprdMIHEGjuiGu66GW+ljBCSevKAbJloMX9qxZftCU3r0y75rE7sY4wOL6p9YBCXGDKGBQw+dnqvK5i0KBgZ+Uy1bd0W/fl42T7k/ksWtK9B/uSssVJx1TLtYIgxtdTQ9mcG1vKh2zdUm43/VQoT6A09Ynpa9m0KdWyTY2Xfqrdqu1vm6GSkdcILRi6ZASdUbf8QPbSvF/yxANZaJ4RWnEKRmpDkJoCqN2WtQx/M2lVthn9TojjfViMWem1pgp/XhuwoIqvwzg8n+bapKJyaSnPUq/2SPvc9c7iTkB0fPqRTFEZ0IJ+lSBI/J5YCaQVCFYcGblS8Gr8icjLdIlKlYVcJqYhvt3HfwQ0AJnw2+tGcfErKEOuaGiHBcYqa7W9KGQvVz6tjrR3FmFU/W/bEI3s6oFUZg9VCOJAwFT8IV7E1sZszX/CZaK7CUgM4B5+I270Uf7QfP1vvv7IndUnGZZ3ie63rtg+IW+Z8d+SaK5h5tKu8eLegdPqilKHOnG2D6MDEYOLyaAF/aHkcXP7FWQ+SNUtUMbAapAui+rKnyJMDyyCZ0WflOpqoT+l0d2puaVYTskO/JM55XtiEE5kVcuVCHs+ZaOvDPa35PUy9NW9dRICjJJSoknJgaVRs4QGhdTtOcFjqt7R0Aavc0RqPXOfr8m+xG2ZNO3Ex1E1oHMgfXA81NF/IPYRDYYJ+ZGd0WKx3sEm2xSX0LSIsDVXvQfz2k+l9Kep/OiIQsAYMw4XrZ0i/NkE4DtEs6NNV7vry4FoMZIdAPguTofhpAKViTDhtVlZZFrayBUO0qSEiVc9XSpTNRirXZTfH9mu6/5jc22l7hFNy5u6Ujf1GHZ4SJAA0YCXt1tpKULfEV9JK+HHHKFxMVv5NqlsrEIuVKnL0Xm/DRoT4HeV7Q4nhY1/8VZm8WuUZoCYOcuHerPMkUkJgcemVviz+yIbVmjHY/zhMX9zhPEio7XKzPUgj/53CxpYt09oOjcyrVCnXs+Iai1XelkcMqGSiapGwQx5ixtoVEptznFEOILHcUJG562fElxXmq7SdZS4gyK3+M58Yk+IqAPuDGuJZCGvocRnb3/dPseVh8hWZId3Ui54UrOl0YMwS'}]}}], 'usage': {'prompt_tokens': 216, 'completion_tokens': 1219, 'total_tokens': 1435, 'cost': 0.00183714, 'is_byok': False, 'prompt_tokens_details': {'cached_tokens': 192, 'audio_tokens': 0, 'video_tokens': 0}, 'cost_details': {'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 8.64e-06, 'upstream_inference_completions_cost': 0.0018285}, 'completion_tokens_details': {'reasoning_tokens': 366, 'image_tokens': 0}}}\n"
          ]
        }
      ],
      "source": [
        "print(response.json())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpSwMq6cjqXv"
      },
      "source": [
        "# deepsee ocr --- poc test with bunch of library ( need to work)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5af748Cahd7f",
        "outputId": "78f37fe9-7191-4675-d538-fcf505b704b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.12/dist-packages (2.6.0)\n",
            "Collecting torchvision==0.19.0\n",
            "  Downloading torchvision-0.19.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: transformers==4.46.3 in /usr/local/lib/python3.12/dist-packages (4.46.3)\n",
            "Requirement already satisfied: tokenizers==0.20.3 in /usr/local/lib/python3.12/dist-packages (0.20.3)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (0.8.1)\n",
            "Requirement already satisfied: addict in /usr/local/lib/python3.12/dist-packages (2.4.0)\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.12/dist-packages (1.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (4.15.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (3.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (75.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (1.13.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.19.0) (2.0.2)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Cannot install torch==2.6.0 and torchvision==0.19.0 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "The conflict is caused by:\n",
            "    The user requested torch==2.6.0\n",
            "    torchvision 0.19.0 depends on torch==2.4.0\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
            "\n",
            "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install torch==2.6.0 torchvision==0.19.0 transformers==4.46.3 tokenizers==0.20.3 einops addict easydict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcq3ATeAhz7e",
        "outputId": "03a8f4b5-37c9-445d-ee1a-bb58cfe72405"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting flash-attn==2.7.3\n",
            "  Using cached flash_attn-2.7.3.tar.gz (3.2 MB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "!pip install flash-attn==2.7.3 --no-build-isolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "P5uvfO1KgdAf",
        "outputId": "1939367e-689f-4c74-dd6b-cb6cf01580dd"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
        "model_name = 'deepseek-ai/DeepSeek-OCR'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "model = AutoModel.from_pretrained(model_name, trust_remote_code=True, use_safetensors=True)\n",
        "model = model.eval().cuda().to(torch.bfloat16)\n",
        "\n",
        "# prompt = \"<image>\\nFree OCR. \"\n",
        "prompt = \"<image>\\n<|grounding|>Convert the document to markdown. \"\n",
        "image_file = '/content/WhatsApp Image 2025-10-31 at 10.53.17 PM.jpeg'\n",
        "output_path = 'your/output/dir'\n",
        "\n",
        "# infer(self, tokenizer, prompt='', image_file='', output_path = ' ', base_size = 1024, image_size = 640, crop_mode = True, test_compress = False, save_results = False):\n",
        "\n",
        "# Tiny: base_size = 512, image_size = 512, crop_mode = False\n",
        "# Small: base_size = 640, image_size = 640, crop_mode = False\n",
        "# Base: base_size = 1024, image_size = 1024, crop_mode = False\n",
        "# Large: base_size = 1280, image_size = 1280, crop_mode = False\n",
        "\n",
        "# Gundam: base_size = 1024, image_size = 640, crop_mode = True\n",
        "\n",
        "res = model.infer(tokenizer, prompt=prompt, image_file=image_file, output_path = output_path, base_size = 1024, image_size = 640, crop_mode=True, save_results = True, test_compress = True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
